{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 1) Tokenizer 생성하기\n",
    "### 1-1 preprocessing()\n",
    "- 텍스트 전처리 하는 함수\n",
    "\n",
    "### 1-2 fit()\n",
    "- 어휘 사전을 구축하는 함수\n",
    "\n",
    "### 1-3 transform()\n",
    "- 어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "  def __init__(self):\n",
    "    self.word_dict = {'oov': 0}\n",
    "    self.fit_checker = False\n",
    "  \n",
    "  def preprocessing(self, sequences):\n",
    "    result = []\n",
    "    result = [sequence.lower() for sequence in sequences] #소문자 변환\n",
    "    result = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', sequence).split() for sequence in result] # 특수문자 제거, 공백 기준 토큰화\n",
    "    return result\n",
    "\n",
    "  def fit(self, sequences):\n",
    "    self.fit_checker = False\n",
    "    docs = self.preprocessing(sequences) #각 문장에 대해 토큰화 수행\n",
    "    for words in docs: \n",
    "        for token in words:\n",
    "            self.word_dict[token] = self.word_dict.get(token, len(self.word_dict))\n",
    "    self.fit_checker = True\n",
    "\n",
    "  def transform(self, sequences):\n",
    "    result = []\n",
    "    tokens = self.preprocessing(sequences)\n",
    "    if self.fit_checker:\n",
    "        for s in tokens:\n",
    "            temp = []\n",
    "            for word in s:\n",
    "                try:\n",
    "                    temp.append(self.word_dict[word]) #어휘사전에 있는 단어 정수 인덱싱\n",
    "                except KeyError:\n",
    "                    temp.append(self.word_dict['oov']) #어휘사전에 없는 단어 변환\n",
    "            result.append(temp)\n",
    "        return result\n",
    "    else:\n",
    "        raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
    "        \n",
    "  def fit_transform(self, sequences):\n",
    "    self.fit(sequences)\n",
    "    result = self.transform(sequences)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 2) Tfidf Vectorizer 생성하기\n",
    "Tfidf - 문서에서 단어빈도-역문서빈도를 수치화한 중요도 스코어\n",
    "\n",
    "### 2-1 fit()\n",
    "- 입력 문장들을 이용해 IDF 행렬 생성\n",
    "\n",
    "### 2-2 transform()\n",
    "- 입력 문장들을 이용해 TF-IDF 행렬 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "  def __init__(self, tokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.fit_checker = False\n",
    "    self.idf_lst = []\n",
    "  \n",
    "  def fit(self, sequences):\n",
    "    tokenized = self.tokenizer.fit_transform(sequences)\n",
    "    count_dict = dict(Counter(item for items in tokenized for item in items))\n",
    "    for key in count_dict.keys():\n",
    "        #전체 문장 개수를 분자로, 단어 key가 포함된 문장의 개수 + 1을 분모로 계산\n",
    "        self.idf_lst.append(np.log(len(tokenized) / (count_dict[key] + 1)))\n",
    "    self.fit_checker = True\n",
    "\n",
    "  def transform(self, sequences):\n",
    "    if self.fit_checker:\n",
    "        tokenized = self.tokenizer.transform(sequences)\n",
    "        count_dict = dict(Counter(item for items in tokenized for item in items))\n",
    "        matrix = np.zeros((len(sequences), len(count_dict))) #문장, 토큰 수만큼 행렬 생성\n",
    "        for i, words in enumerate(tokenized):\n",
    "            for word in words:\n",
    "                matrix[i, word-1] += 1\n",
    "        self.tfidf_matrix = matrix * self.idf_lst\n",
    "        return self.tfidf_matrix\n",
    "    else:\n",
    "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
    "    \n",
    "  def fit_transform(self, sequences):\n",
    "    self.fit(sequences)\n",
    "    return self.transform(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트\n",
    "lst = ['I go to school.', 'I LIKE pizza!', 'i to', 'journey']\n",
    "tk = Tokenizer()\n",
    "tfv = TfidfVectorizer(tk)\n",
    "x = tfv.fit_transform(lst)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718, 0.28768207, 0.69314718, 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.69314718,\n",
       "        0.69314718, 0.        ],\n",
       "       [0.        , 0.        , 0.28768207, 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.69314718]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
